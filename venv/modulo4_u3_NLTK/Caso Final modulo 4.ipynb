{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso Final 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: GTK3Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagger training para el corpues en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "sents = cess_esp.tagged_sents()\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la practica 2 del modulo hemos comprobado el tagger mas completo\n",
    ", asi que para este caso final se va a usar HiddenMarkovModelTagger.Para entrenar el POS tagger vamos a dividir los datos en entrenamiento y en test. El 90% sera de entrenamiento y el 10% sera con el que evaluemos nuestro tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: 89.88905831011094\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "testing = []\n",
    "\n",
    "for i in range(len(sents)) :\n",
    "    if i % 10 :\n",
    "        training.append(sents[i])\n",
    "    else :\n",
    "        testing.append(sents[i])\n",
    "        \n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "spanish_pos_tagger = HiddenMarkovModelTagger.train(training)\n",
    "\n",
    "print('Evaluation:', spanish_pos_tagger.evaluate(testing)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos MarkovMOdel tagger para nuestro analisi morfologico ya que no se ve sobreajustado y la evaluacion es alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos tagset para el corpus en español\n",
    "\n",
    "Observamos todas las etiquetas que nos porporciona nuestro tagger para el español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yo', 'pp1csn00'),\n",
       " ('muchas', 'rn'),\n",
       " ('quiero', 'vmip1s0'),\n",
       " ('una', 'di0fs0'),\n",
       " ('pizza', 'ncfs000'),\n",
       " ('de', 'sps00'),\n",
       " ('cuatro', 'dn0cp0'),\n",
       " ('quesos', 'ncmp000'),\n",
       " (',', 'Fc'),\n",
       " ('y', 'cc'),\n",
       " ('tres', 'dn0cp0'),\n",
       " ('hamburguesas', 'ncmp000'),\n",
       " ('con', 'sps00'),\n",
       " ('patatas', 'ncfp000')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'yo quiero una pizza de cuatro quesos, y tres hamburguesas con patatas'\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = spanish_pos_tagger.tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos obeservar las etiquetas usadas y crear la gramatica para el regexparser. Para poder crear el Regex parser se ha tenido que investigar y entender las etiquetas para el corpus en español. Se ha usado la referencia siguiente: https://www.sketchengine.eu/spanish-freeling-part-of-speech-tagset/ . Ademas, para crear posteriormente nuestro regexparser es importante entender las etiquetas. Para hacer bien la fragmentacion, en ese caso se ha practicado con expresiones regulares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp1csn00\n",
      "None <_sre.SRE_Match object; span=(4, 8), match='sn00'> None\n",
      "rn\n",
      "None None None\n",
      "vmip1s0\n",
      "None <_sre.SRE_Match object; span=(5, 7), match='s0'> None\n",
      "di0fs0\n",
      "None <_sre.SRE_Match object; span=(4, 6), match='s0'> None\n",
      "ncfs000\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ncfs000'> <_sre.SRE_Match object; span=(3, 7), match='s000'> <_sre.SRE_Match object; span=(0, 2), match='nc'>\n",
      "sps00\n",
      "None <_sre.SRE_Match object; span=(0, 5), match='sps00'> None\n",
      "dn0cp0\n",
      "None None None\n",
      "ncmp000\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ncmp000'> None <_sre.SRE_Match object; span=(0, 2), match='nc'>\n",
      "Fc\n",
      "None None None\n",
      "cc\n",
      "None None None\n",
      "dn0cp0\n",
      "None None None\n",
      "ncmp000\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ncmp000'> None <_sre.SRE_Match object; span=(0, 2), match='nc'>\n",
      "sps00\n",
      "None <_sre.SRE_Match object; span=(0, 5), match='sps00'> None\n",
      "ncfp000\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ncfp000'> None <_sre.SRE_Match object; span=(0, 2), match='nc'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tags= [tag for word, tag in tagged]\n",
    "for tag in tags:\n",
    "    print(tag)\n",
    "    x = re.search(\"s?n[cp\\.].*\", tag)  #nombre propio o comun.\n",
    "    y = re.search(\"s.*\", tag)       #sn.e-SUJ\n",
    "    z =re.search(\"s?[na][cp]\",tag)   #anadimos el adjetivo\n",
    "    print(x ,y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp1csn00\n",
      "None None\n",
      "rn\n",
      "None None\n",
      "vmip1s0\n",
      "None None\n",
      "di0fs0\n",
      "<_sre.SRE_Match object; span=(0, 6), match='di0fs0'> None\n",
      "ncfs000\n",
      "None None\n",
      "sps00\n",
      "None None\n",
      "dn0cp0\n",
      "<_sre.SRE_Match object; span=(0, 6), match='dn0cp0'> None\n",
      "ncmp000\n",
      "None None\n",
      "Fc\n",
      "None None\n",
      "cc\n",
      "None None\n",
      "dn0cp0\n",
      "<_sre.SRE_Match object; span=(0, 6), match='dn0cp0'> None\n",
      "ncmp000\n",
      "None None\n",
      "sps00\n",
      "None None\n",
      "ncfp000\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "tags= [tag for word, tag in tagged]\n",
    "for tag in tags:\n",
    "    print(tag)\n",
    "    x = re.search(\"d[in].*\", tag)  #determiner : dn0cp0  indefinit \n",
    "    y = re.search(\"Z.*\", tag)       #number\n",
    "    print(x ,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version1. Regex parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    Comida: {<s?n[cp\\.].*><s.*>?<s?[na][cp\\.].*>*}\n",
    "            {<s?n[cp\\.].*>}\n",
    "    Cantidad: {<d[in].*>}\n",
    "              {<Z.*>}  \n",
    "\"\"\"\n",
    "\n",
    "regex_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  yo/pp1csn00\n",
      "  quiero/vmip1s0\n",
      "  (Cantidad 3/di0fp0)\n",
      "  (Comida bocadillos/ncfp000 con/sps00 cebolla/np0000l)\n",
      "  ,/Fc\n",
      "  (Cantidad 3/Z)\n",
      "  (Comida pizzas/ncmp000)\n",
      "  y/cc\n",
      "  (Comida ensalada/sn.e-SUJ))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('yo', 'pp1csn00', 'O'),\n",
       " ('quiero', 'vmip1s0', 'O'),\n",
       " ('3', 'di0fp0', 'B-Cantidad'),\n",
       " ('bocadillos', 'ncfp000', 'B-Comida'),\n",
       " ('con', 'sps00', 'I-Comida'),\n",
       " ('cebolla', 'np0000l', 'I-Comida'),\n",
       " (',', 'Fc', 'O'),\n",
       " ('3', 'Z', 'B-Cantidad'),\n",
       " ('pizzas', 'ncmp000', 'B-Comida'),\n",
       " ('y', 'cc', 'O'),\n",
       " ('ensalada', 'sn.e-SUJ', 'B-Comida')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'yo quiero 3 bocadillos con cebolla, 3 pizzas y ensalada'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = spanish_pos_tagger.tag(tokens)\n",
    "chunked = regex_parser.parse(tagged)\n",
    "print(chunked)\n",
    "nltk.chunk.tree2conlltags(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la practica se pide mostrar la comida y la cantidad por separado. Asi que se crea una funcion llamada pedir_comida para que se muestre en forma de diccionario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " def pedir_comida(chunked_sentence) :\n",
    "    pedido = []\n",
    "    dict_ = dict()\n",
    "    default_cant = 1 #por defecto descrita en el ejercicio\n",
    "    #list comprehension. incluimos todas aquellas palabras que tengan comida o cantidad.\n",
    "    chunks = [chunk for chunk in nltk.chunk.tree2conlltags(chunked_sentence) \n",
    "                  if 'Cantidad' in chunk[2] or 'Comida' in chunk[2]]\n",
    "    \n",
    "    for i in range(0,len(chunks)):\n",
    "        chunk = chunks[i]\n",
    "        word, tag, chunk_type = chunk\n",
    "        \n",
    "        if chunk_type == 'B-Cantidad' :\n",
    "            dict_['cantidad'] = word\n",
    "\n",
    "        if chunk_type == 'B-Comida' :\n",
    "            if 'cantidad' not in dict_ :\n",
    "                dict_['cantidad'] = default_cant\n",
    "\n",
    "            dict_['comida'] = word\n",
    "            pedido.append(dict_)\n",
    "            dict_ = {}\n",
    "        \n",
    "    return pedido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quisiera cuatro solomillos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cantidad': '3', 'comida': 'bocadillos'},\n",
       " {'cantidad': '3', 'comida': 'pizzas'},\n",
       " {'cantidad': 1, 'comida': 'ensalada'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentence)\n",
    "pedir_comida(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 . Unigramchunker y BigramChunker y NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los chunkers, lo primero que hay que hacer es crear un corpus en formato IOB. Para obtener el IOB tag de una nueva frase, utilizaremos el pos_tagger_spanish y el regexparser creados anteriormente. El formato IOB es el formato estandard para representar estructuras fragmentadas(chunked structures), además NLTK da metodos para leer y escribir en dicho formato. Por eso debemos hacer la conversion. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus():\n",
    "    corpus_pedidos = ['Quisiera pedir un hamburguesa', 'Quiero una tortilla y una cerveza',\n",
    "                     'Me pones un pollo y una ensalada', 'Quiero una paella', 'Quiero un bocadillo', \n",
    "                     'Quiero una pizza', 'Ponme una sopa', 'Quiero un filete', 'Quisiera pedir una ensalada',\n",
    "                     'Quiero cinco bocadillos', 'Quisiera una empanada', 'Quiero unas croquetas', 'Quisiera morcilla', \n",
    "                     'Quiero pedir un solomillo', 'Quiero unos macarrones', 'Quiero una Lasagna', 'Quiero una hamburguesa', \n",
    "                     ' una de patatas fritas y una cerveza', 'Quiero un lenguado', 'Quiero un bonito', 'Quisiera una sepia',\n",
    "                     'Quiero cinco cervezas', 'Quiero tres sidras y tres pinchos', 'Quiero cinco manzanas y tres melocotones',\n",
    "                     'Quisiera cuatro solomillos', 'Quiero una naranja y dos peras', 'Quisiera pedir un hamburguesa',\n",
    "                     'Me gustaría comer una tortilla de patatas', '¿Nos pones 3 tocinos, 4 pechugas?',\n",
    "                     '5 repollos, 12 sardinas y 8 jalapeños',\n",
    "                     'Estamos indecisos, pero puedes ponernos mientras tres raciones de patatas fritas',\n",
    "                     '¿Sería tan amable de servirnos catorce tostadas?',\n",
    "                     'Queremos 2 pollos con caracoles, una ternera, tres patos y 2 pimientos con arroz', \n",
    "                     'Nuestro pedido es: 10 hamburguesas con queso, 20 pizzas y 3 patatas',\n",
    "                     'Yo quiero 9 yogures, 12 ensaladas, un pimiento, 4 chorizos, 12 empanadillas de atún, 8 crespillos y 3 alubias',\n",
    "                     'Me gustaría comer: kiwi con ensalada', 'Él quiere pedir 3 tostadas, 2 tomates y 6 uvas',\n",
    "                     'Ellos han pedido 2 higos, 1 salmón con caracoles y arroz', \n",
    "                     'Tú has pedido 4 fajitas de pollo, dos emperadores con patatas y tres rúculas', \n",
    "                     '2 macarrones, 3 quesos, una guindilla, cuatro pulpos y quince verduras', \n",
    "                     'Mi abuela quiere emperador con ensalada', 'Mi tía quiere comer marisco y carne',\n",
    "                     '2 cebollas, una calabaza y ocho tomates', 'tres pollos, dos corderos, cuatro cerdos y tres macarrones',\n",
    "                     'Pepinillos, calabaza, navajas, sepia y gulas', 'Me gustaría comer bonito a la plancha con patatas asadas', \n",
    "                     'Pídete cinco pomelos, tres calabacines, 8 mangos, seis melocotonesLangosta, langostinos y almejas']\n",
    "      \n",
    "    return corpus_pedidos\n",
    "\n",
    "len(load_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iob_corpus(corpus, regex_parser) :\n",
    "    iob_corpus_pedidos = []\n",
    "    for sent in corpus :\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tagged_sentence = spanish_pos_tagger.tag(tokens)\n",
    "        chunked_sentence = regex_parser.parse(tagged_sentence)\n",
    "        iob_corpus_pedidos.append(chunked_sentence)\n",
    "        \n",
    "    return iob_corpus_pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_corpus = iob_corpus(load_corpus(), regex_parser)\n",
    "#print(iob_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos las etiquetas en formato iob, podremos seguir los pasos del capítulo 7 del NLTK book. Usando las clases de Unigram y BigramChunker primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data) \n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data) \n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar los chunkers, tenemos que dividir nuestro corpus como hemos hecho anteriormente con el cess_esp. el corpus tiene 47 frases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = iob_corpus[0:40]\n",
    "testing = iob_corpus[40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar el UnigramChunker y los datos test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  95.5%%\n",
      "    Precision:     90.9%%\n",
      "    Recall:        93.8%%\n",
      "    F-Measure:     92.3%%\n",
      "\n",
      "Quisiera pedir una ensalada\n",
      "[{'cantidad': 'una', 'comida': 'ensalada'}]\n"
     ]
    }
   ],
   "source": [
    "unigram_chunker = UnigramChunker(training)\n",
    "print(unigram_chunker.evaluate(testing))\n",
    "\n",
    "sentence = random.choice(load_corpus())\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = spanish_pos_tagger.tag(tokens)\n",
    "\n",
    "print()\n",
    "print(sentence)\n",
    "print(pedir_comida(unigram_chunker.parse(tagged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  76.1%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:        59.4%%\n",
      "    F-Measure:     74.5%%\n",
      "\n",
      "Me pones un pollo y una ensalada\n",
      "[{'cantidad': 'un', 'comida': 'pollo'}, {'cantidad': 'una', 'comida': 'ensalada'}]\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(training)\n",
    "print(bigram_chunker.evaluate(testing))\n",
    "sentence=random.choice(load_corpus())\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = spanish_pos_tagger.tag(tokens)\n",
    "\n",
    "print()\n",
    "print(sentence)\n",
    "print(pedir_comida(bigram_chunker.parse(tagged)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para observar diferentes Chunkers, podemos observar y evaluar el NaiveBayesChunker. En el apartado 3.3 del capitulo 7 , se explica la formacion de chunkers basados en clasificadores. Para maximizar el rendimiento de la fragmentacion, necesitamos informacion del contenido de las palabras a parte de sus etiquetas. Por ese motivo usamos ConsecutivePosTagger y NaiveBayesClassifier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "                \n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "    \n",
    "    \n",
    "class NaiveBayesChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutivePosTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.6%%\n",
      "    Precision:     81.8%%\n",
      "    Recall:        84.4%%\n",
      "    F-Measure:     83.1%%\n",
      "\n",
      "tres pollos, dos corderos, cuatro cerdos y tres macarrones\n",
      "\n",
      "[{'cantidad': 'tres', 'comida': 'pollos'}, {'cantidad': 'dos', 'comida': 'corderos'}, {'cantidad': 'cuatro', 'comida': 'cerdos'}, {'cantidad': 'tres', 'comida': 'macarrones'}]\n"
     ]
    }
   ],
   "source": [
    "naive_chunker = NaiveBayesChunker(training)\n",
    "print(naive_chunker.evaluate(testing))\n",
    "\n",
    "sentence = random.choice(load_corpus())\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = spanish_pos_tagger.tag(tokens)\n",
    "\n",
    "parsed = naive_chunker.parse(tagged)\n",
    "\n",
    "print()\n",
    "print(sentence)\n",
    "print()\n",
    "print(pedir_comida(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    \n",
    "    return { \" pos \" : pos, \" word \" : word,  \" prevpos \" : prevpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  86.6%%\n",
      "    Precision:     75.8%%\n",
      "    Recall:        78.1%%\n",
      "    F-Measure:     76.9%%\n",
      "\n",
      "Quisiera cuatro solomillos\n",
      "\n",
      "[{'cantidad': 'cuatro', 'comida': 'solomillos'}]\n"
     ]
    }
   ],
   "source": [
    "naive_chunker = NaiveBayesChunker(training)\n",
    "print(naive_chunker.evaluate(testing))\n",
    "\n",
    "sentence = random.choice(load_corpus())\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = spanish_pos_tagger.tag(tokens)\n",
    "\n",
    "parsed = naive_chunker.parse(tagged)\n",
    "\n",
    "print()\n",
    "print(sentence)\n",
    "print()\n",
    "print(pedir_comida(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramChunker\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  95.5%%\n",
      "    Precision:     90.9%%\n",
      "    Recall:        93.8%%\n",
      "    F-Measure:     92.3%%\n",
      "BigramChunker\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  76.1%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:        59.4%%\n",
      "    F-Measure:     74.5%%\n",
      "NaiveBayesChunker (token actual como unigram chunker)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  86.6%%\n",
      "    Precision:     75.8%%\n",
      "    Recall:        78.1%%\n",
      "    F-Measure:     76.9%%\n",
      "NaiveBayesChunker(token actual y previo )\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  86.6%%\n",
      "    Precision:     75.8%%\n",
      "    Recall:        78.1%%\n",
      "    F-Measure:     76.9%%\n"
     ]
    }
   ],
   "source": [
    "print(\"UnigramChunker\")\n",
    "print(unigram_chunker.evaluate(testing))\n",
    "print(\"BigramChunker\")\n",
    "print(bigram_chunker.evaluate(testing))\n",
    "naive_chunker = NaiveBayesChunker(training)\n",
    "print(\"NaiveBayesChunker (token actual como unigram chunker)\")\n",
    "print(naive_chunker.evaluate(testing))\n",
    "naive_chunker = NaiveBayesChunker(training)\n",
    "print(\"NaiveBayesChunker(token actual y previo )\")\n",
    "print(naive_chunker.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las evaluaciones anteriores podemos observar que UnigramChunker y NaiveBayeschunker son los mejores para nuestro corpus de entrenamiento. Es cierto que nuestro corpus al ser pequeño pude crear sobreajuste y estas evaluaciones no son fiables.\n",
    "\n",
    "Para el NaiveBayesChunker tenemos que recordar que usando el POS tag previo, estamos usando el teorema de bayes y multiplicando la probabilidad de palabras conjuntas. Al tener un corpus_pedido pequeño y poco variado, no podemos dar  por validos los resultados. En teoria, NaiveBayesChunker es el mejor. \n",
    "\n",
    "Para el Bigramtagger pasa lo mismo. Si miramos las etiquetas anteriores con tan pocos ejemplos, no los podemos entrenar debidamente. \n",
    "\n",
    "Podemos concluir que para esta practica nos quedariamos con Unigram o Naivebayes. Siempre recordando que hemos usado un corpus de variedad my escaso y pequeño.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
